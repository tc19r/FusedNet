{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpMW/uZTBHgy9C5NFy2Tvn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tc19r/FusedNet/blob/main/unit_tes_of_fusednet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcVypPzJ2ezl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "from tensorflow.python.keras.layers import concatenate\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from memory_profiler import profile\n",
        "import numpy as np\n",
        "\n",
        "@profile\n",
        "def run():\n",
        "    #Load {data -> x_train, y_train}\n",
        "    X=np.empty([0,34])\n",
        "    rootpath='./'\n",
        "    for root, dirs, files in os.walk(rootpath):\n",
        "         for filename in [fi for fi in files if fi.endswith(\".pf\")]:\n",
        "             pf=np.fromfile(filename)\n",
        "             pf=pf.reshape(int(len(pf)/34),34)\n",
        "             np.random.shuffle(pf)\n",
        "             X=np.vstack([X,pf])\n",
        "             print(pf.shape)\n",
        "             print(filename)\n",
        "    trainset=np.float64(X)\n",
        "\n",
        "    num_classes=2\n",
        "    dim_fib=30\n",
        "    from sklearn.mixture import GaussianMixture\n",
        "    gm = GaussianMixture(n_components=num_classes, random_state=0,warm_start=True,max_iter=10)\n",
        "    dataset=trainset[:,0:8]\n",
        "    #Run\n",
        "    #Random sampling, Acts as shufflling when shrink = 1  \n",
        "    shrink=1\n",
        "    indexes=np.random.choice(dataset.shape[0], int(np.floor(dataset.shape[0]/shrink)), replace=False)\n",
        "    x_train=trainset[indexes,:]\n",
        "    y_train=trainset[indexes,0:dim_fib]\n",
        "    \n",
        "    #Hyper parameters \n",
        "    learningrate,lambda1,lambda2=0.0003,1.5e-2,3e-4\n",
        "    epochs = 10\n",
        "    rootpath=\"./\"\n",
        "    num_classes=2\n",
        "    Centers_Lengths_dimension=4\n",
        "    fibs_input_dim=30\n",
        "    dim_fib=30\n",
        "    dim_ctnl=4\n",
        "    recon_dim=fibs_input_dim\n",
        "    inputs_center_and_length = tf.keras.Input(shape=(Centers_Lengths_dimension,))\n",
        "    inputs_fibs_on_ball = tf.keras.Input(shape=(fibs_input_dim,))\n",
        "    encoding_dim = 4\n",
        "    \n",
        "    def getmodel():\n",
        "        encoder=keras.layers.Dense(16, activation='tanh')(inputs_fibs_on_ball)\n",
        "        encoder=keras.layers.Dense(8, activation='tanh')(encoder)\n",
        "        encoder=keras.layers.Dense(4, activation='tanh')(encoder)\n",
        "        encoder=keras.layers.Dense(encoding_dim, activation='tanh',name=\"feature\")(encoder)\n",
        "        encoder=keras.Model(inputs=inputs_fibs_on_ball, outputs=encoder)\n",
        "\n",
        "\n",
        "        decoder=keras.layers.Dense(4,activation='tanh')(encoder.outputs[0])\n",
        "        decoder=keras.layers.Dense(8, activation='tanh')(decoder)\n",
        "        decoder=keras.layers.Dense(16,activation='tanh')(decoder)\n",
        "        decoder=keras.layers.Dense(recon_dim,activation='linear')(decoder)\n",
        "        decoder=keras.Model(inputs=encoder.inputs, outputs=decoder)\n",
        "\n",
        "        combined = concatenate([decoder.output, encoder.output,inputs_center_and_length,],axis=1)\n",
        "        fusednet = keras.Model(inputs=[inputs_fibs_on_ball,inputs_center_and_length],outputs=combined)\n",
        "        feature_extractor = keras.Model(\n",
        "           inputs=inputs_fibs_on_ball,\n",
        "           outputs=fusednet.get_layer(name=\"feature\").output,\n",
        "        )\n",
        "        return fusednet,feature_extractor,\n",
        "    fusednet,feature_extractor = getmodel()\n",
        "\n",
        "    \n",
        "    import time\n",
        "    model=fusednet\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learningrate)\n",
        "    #would the trainning metric made the update faster? scope?\n",
        "    for epoch in range(epochs):\n",
        "      print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "      start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "      with tf.GradientTape() as tape:\n",
        "          y_pred = model([x_train[:,0:dim_fib],x_train[:,dim_fib:dim_fib+dim_ctnl]], training=True)\n",
        "          z = y_pred[:,recon_dim:]\n",
        "          gm.fit(z)\n",
        "          gamma = gm.predict_proba(z)\n",
        "          gamma_sum = tf.reduce_sum(gamma, axis=0)\n",
        "          phi = tf.reduce_mean(gamma, axis=0)\n",
        "          mu = gm.means_\n",
        "          Sigma = gm.covariances_\n",
        "          n_features = z.shape[1]\n",
        "          min_vals = tf.linalg.diag(tf.ones(n_features, dtype=tf.float64)) * 1e-6\n",
        "          z_centered = z[:, None, :] - mu[None, :, :]\n",
        "          L = tf.linalg.cholesky(Sigma + min_vals[None, :, :])\n",
        "          log_det_sigma = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=1)\n",
        "          v = tf.linalg.triangular_solve(L, tf.transpose(z_centered, [1, 2, 0]))\n",
        "          d = z_centered.shape[1]\n",
        "          logits = tf.dtypes.cast(tf.math.log(phi[:, None]), dtype=tf.float64) - 0.5 * (\n",
        "                        tf.dtypes.cast(tf.reduce_sum(tf.square(v), axis=1), dtype=tf.float64) + tf.dtypes.cast(\n",
        "                    d * tf.math.log(2.0 * np.pi), dtype=tf.float64) + tf.dtypes.cast(log_det_sigma[:, None],dtype=tf.float64))\n",
        "          energies = - tf.reduce_logsumexp(logits, axis=0)\n",
        "          enerloss = tf.reduce_mean(energies)\n",
        "          input = y_train[:, 0:recon_dim]\n",
        "          recon = y_pred[:,0:recon_dim]\n",
        "          reconloss = tf.math.reduce_sum(tf.reduce_mean(tf.square(recon - input), axis=-1))\n",
        "          loss_value=lambda1*enerloss + reconloss\n",
        "          grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "          optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "      print(\"Loss:\",float(loss_value))\n",
        "      print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "    #save model\n",
        "    fusednet.save(\"model.h5\")\n",
        "    #save gmm\n",
        "    import pickle\n",
        "    with open('gm.pkl','wb') as f:\n",
        "      pickle.dump(gm,f)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Just get the visualization down too cause bored.\n",
        "\n",
        "#6/9/2022: \n",
        "#environmental\n",
        "import nibabel\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#Load data\n",
        "import os\n",
        "rootpath='./'\n",
        "X=np.empty([0,34])\n",
        "for root, dirs, files in os.walk(rootpath):\n",
        "     for filename in [fi for fi in files if fi.endswith(\".pf\")]:\n",
        "        pf=np.fromfile(filename)\n",
        "        pf=pf.reshape(int(len(pf)/34),34)\n",
        "        np.random.shuffle(pf)\n",
        "        X=np.vstack([X,pf])\n",
        "        print(pf.shape)\n",
        "trainset=np.float64(X)\n",
        "x_train=trainset\n",
        "\n",
        "#parameters\n",
        "\n",
        "#Load Model\n",
        "from tensorflow import keras\n",
        "loaded = keras.models.load_model(\"model.h5\")\n",
        "\n",
        "#Load gmm:\n",
        "import pickle\n",
        "with open('gm.pkl', 'rb') as f:\n",
        "    gm = pickle.load(f)\n",
        "\n",
        "#Get features:\n",
        "feature_extractor = keras.Model(\n",
        "inputs=loaded.inputs,\n",
        "outputs=loaded.get_layer(name=\"feature\").output,)\n",
        "fet=feature_extractor.predict([x_train[:,0:30],x_train[:,30:34]]) #?\n",
        "fullfet=np.hstack([fet,x_train[:,30:34]])\n",
        "\n",
        "##Create bundles:\n",
        "#retrieve labels from gmm\n",
        "#gm = GaussianMixture(n_components, random_state=123).fit(fullfet)\n",
        "labels=gm.predict(fullfet)\n",
        "activeclasses=np.unique(labels)\n",
        "#Acquire classes membership\n",
        "index=[]\n",
        "for cl in activeclasses:\n",
        "    index.append(tf.where(labels==cl).numpy())\n",
        "\n",
        "#Old bundleing routines\n",
        "#preload .trk first\n",
        "from dipy.io.streamline import load_trk\n",
        "# trkpath='./\n",
        "# for root, dirs, files in os.walk(rootpath):\n",
        "#      for filename in [fi for fi in files if fi.endswith(\".pf\")]:\n",
        "#         trkname=os.path.join(trkpath,filename[0:6])\n",
        "#         trkname_list.append(trkname)\n",
        "ct1=load_trk('CC_3.trk', \"same\", bbox_valid_check=False)\n",
        "streamlines=ct1.streamlines\n",
        "\n",
        "#create new bundles\n",
        "o_c_i=[]\n",
        "for i in range(len(activeclasses)):\n",
        "    #o_c_i.append(index[i][np.where((index[i])<X.shape[0])[0]])\n",
        "    o_c_i.append(index[i])\n",
        "\n",
        "from dipy.io.streamline import load_tractogram, save_tractogram\n",
        "for j in range(len(activeclasses)):\n",
        "    nasq=streamlines[o_c_i[j][:,0]]\n",
        "    newSft=ct1\n",
        "    newSft.streamlines=nasq\n",
        "    save_tractogram(newSft, './Pops/cluster_%d.trk' % j, bbox_valid_check=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nT853NCHXM9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPkmyfrzpUcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Losses goes down pretty slowly, 70-~50."
      ],
      "metadata": {
        "id": "sKhDLUdzpq3w"
      }
    }
  ]
}