{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkgoCJa8jd4b+C/pj3dpJ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tc19r/FusedNet/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc2zK9oPn7nf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "from tensorflow.python.keras.layers import concatenate\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from memory_profiler import profile\n",
        "import numpy as np\n",
        "\n",
        "@profile\n",
        "def run():\n",
        "    #Load\n",
        "    X=np.empty([0,34])\n",
        "    rootpath='./'\n",
        "    for root, dirs, files in os.walk(rootpath):\n",
        "         for filename in [fi for fi in files if fi.endswith(\".pf\")]:\n",
        "             pf=np.fromfile(filename)\n",
        "             pf=pf.reshape(int(len(pf)/34),34)\n",
        "             np.random.shuffle(pf)\n",
        "             X=np.vstack([X,pf])\n",
        "             print(pf.shape)\n",
        "             print(filename)\n",
        "    trainset=np.float64(X)\n",
        "\n",
        "    num_classes=400\n",
        "    dim_fib=30\n",
        "    from sklearn.mixture import GaussianMixture\n",
        "    gm = GaussianMixture(n_components=num_classes, random_state=0,warm_start=True,max_iter=10)\n",
        "    dataset=trainset[:,0:8]\n",
        "    #Run\n",
        "    shrink=100\n",
        "    indexes=np.random.choice(dataset.shape[0], int(np.floor(dataset.shape[0]/100)), replace=False)\n",
        "    x_batch_train=trainset[indexes,:]\n",
        "    y_batch_train=trainset[indexes,0:dim_fib]\n",
        "    #Hyper parameters \n",
        "    learningrate,lambda1,lambda2=0.0003,1.5e-2,3e-4\n",
        "    epochs = 50\n",
        "    rootpath=\"./\"\n",
        "    num_classes=400\n",
        "    Centers_Lengths_dimension=4\n",
        "    fibs_input_dim=30\n",
        "    dim_fib=30\n",
        "    dim_ctnl=4\n",
        "    recon_dim=fibs_input_dim\n",
        "    inputs_center_and_length = tf.keras.Input(shape=(Centers_Lengths_dimension,))\n",
        "    inputs_fibs_on_ball = tf.keras.Input(shape=(fibs_input_dim,))\n",
        "    encoding_dim = 4\n",
        "    \n",
        "    def getmodel():\n",
        "        encoder=keras.layers.Dense(16, activation='tanh')(inputs_fibs_on_ball)\n",
        "        encoder=keras.layers.Dense(16, activation='tanh')(encoder)\n",
        "        encoder=keras.layers.Dense(8, activation='tanh')(encoder)\n",
        "        encoder=keras.layers.Dense(4, activation='tanh')(encoder)\n",
        "        encoder=keras.layers.Dense(encoding_dim, activation='tanh',name=\"feature\")(encoder)\n",
        "        encoder=keras.Model(inputs=inputs_fibs_on_ball, outputs=encoder)\n",
        "        #define decoder network:\n",
        "\n",
        "        decoder=keras.layers.Dense(4,activation='tanh')(encoder.outputs[0])\n",
        "        decoder=keras.layers.Dense(8, activation='tanh')(decoder)\n",
        "        #decoder = keras.layers.Dropout(0.25)(decoder)\n",
        "        decoder=keras.layers.Dense(16,activation='tanh')(decoder)\n",
        "        #decoder = keras.layers.Dropout(0.25)(decoder)\n",
        "        decoder=keras.layers.Dense(16,activation='tanh')(decoder)\n",
        "        decoder=keras.layers.Dense(recon_dim,activation='linear')(decoder)\n",
        "        decoder=keras.Model(inputs=encoder.inputs, outputs=decoder)\n",
        "        #===\n",
        "        combined = concatenate([decoder.output, encoder.output,inputs_center_and_length,],axis=1)\n",
        "        fusednet = keras.Model(inputs=[inputs_fibs_on_ball,inputs_center_and_length],outputs=combined)\n",
        "        feature_extractor = keras.Model(\n",
        "           inputs=inputs_fibs_on_ball,\n",
        "           outputs=fusednet.get_layer(name=\"feature\").output,\n",
        "        )\n",
        "        return fusednet,feature_extractor,\n",
        "    fusednet,feature_extractor = getmodel()\n",
        "\n",
        "    model=fusednet\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model([x_batch_train[:,0:dim_fib],x_batch_train[:,dim_fib:dim_fib+dim_ctnl]], training=True)\n",
        "    z = y_pred[:,recon_dim:]\n",
        "    gm.fit(z)\n",
        "    gamma = gm.predict_proba(z)\n",
        "    gamma_sum = tf.reduce_sum(gamma, axis=0)\n",
        "    phi = tf.reduce_mean(gamma, axis=0)\n",
        "    mu = gm.means_\n",
        "    Sigma = gm.covariances_\n",
        "    n_features = z.shape[1]\n",
        "    min_vals = tf.linalg.diag(tf.ones(n_features, dtype=tf.float64)) * 1e-6\n",
        "    z_centered = z[:, None, :] - mu[None, :, :]\n",
        "    L = tf.linalg.cholesky(Sigma + min_vals[None, :, :])\n",
        "    log_det_sigma = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=1)\n",
        "    v = tf.linalg.triangular_solve(L, tf.transpose(z_centered, [1, 2, 0]))\n",
        "    v = tf.linalg.triangular_solve(L, tf.transpose(z_centered, [1, 2, 0]))\n",
        "    log_det_sigma = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=1)\n",
        "    d = z_centered.shape[1]\n",
        "    logits = tf.dtypes.cast(tf.math.log(phi[:, None]), dtype=tf.float64) - 0.5 * (\n",
        "                        tf.dtypes.cast(tf.reduce_sum(tf.square(v), axis=1), dtype=tf.float64) + tf.dtypes.cast(\n",
        "                    d * tf.math.log(2.0 * np.pi), dtype=tf.float64) + tf.dtypes.cast(log_det_sigma[:, None],\n",
        "                                                                                     dtype=tf.float64))\n",
        "    energies = - tf.reduce_logsumexp(logits, axis=0)\n",
        "    enerloss = tf.reduce_mean(energies)\n",
        "    input = y_batch_train[:, 0:recon_dim]\n",
        "    recon = y_pred[:,0:recon_dim]\n",
        "    reconloss = tf.math.reduce_sum(tf.reduce_mean(tf.square(recon - input), axis=-1))\n",
        "    loss_value=lambda1*enerloss + reconloss\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1906337, 34)\n",
        "628248.pf\n",
        "(2126374, 34)\n",
        "898176.pf\n",
        "Filename: ./my_file.py\n",
        "\n",
        "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
        "=============================================================\n",
        "    11    245.9 MiB    245.9 MiB           1   @profile\n",
        "    12                                         def run():\n",
        "    13                                             #Load\n",
        "    14    245.9 MiB      0.0 MiB           1       X=np.empty([0,34])\n",
        "    15    245.9 MiB      0.0 MiB           1       rootpath='./'\n",
        "    16   1843.7 MiB      0.0 MiB           4       for root, dirs, files in os.walk(rootpath):\n",
        "    17   1843.7 MiB      0.0 MiB          36            for filename in [fi for fi in files if fi.endswith(\".pf\")]:\n",
        "    18   1292.1 MiB    551.8 MiB           2                pf=np.fromfile(filename)\n",
        "    19   1292.1 MiB      0.0 MiB           2                pf=pf.reshape(int(len(pf)/34),34)\n",
        "    20   1292.1 MiB      0.0 MiB           2                np.random.shuffle(pf)\n",
        "    21   1843.7 MiB   1046.0 MiB           2                X=np.vstack([X,pf])\n",
        "    22   1843.7 MiB      0.0 MiB           2                print(pf.shape)\n",
        "    23   1843.7 MiB      0.0 MiB           2                print(filename)\n",
        "    24   1843.7 MiB      0.0 MiB           1       trainset=np.float64(X)\n",
        "    25                                         \n",
        "    26   1843.7 MiB      0.0 MiB           1       num_classes=400\n",
        "    27   1843.7 MiB      0.0 MiB           1       dim_fib=30\n",
        "    28   1843.7 MiB      0.0 MiB           1       from sklearn.mixture import GaussianMixture\n",
        "    29   1843.7 MiB      0.0 MiB           1       gm = GaussianMixture(n_components=num_classes, random_state=0,warm_start=True,max_iter=10)\n",
        "    30   1843.7 MiB      0.0 MiB           1       dataset=trainset[:,0:8]\n",
        "    31                                             #Run\n",
        "    32   1843.7 MiB      0.0 MiB           1       shrink=100\n",
        "    33   1874.5 MiB     30.8 MiB           1       indexes=np.random.choice(dataset.shape[0], int(np.floor(dataset.shape[0]/100)), replace=False)\n",
        "    34   1886.1 MiB     11.6 MiB           1       x_batch_train=trainset[indexes,:]\n",
        "    35   1894.1 MiB      8.1 MiB           1       y_batch_train=trainset[indexes,0:dim_fib]\n",
        "    36                                             #Hyper parameters \n",
        "    37   1894.1 MiB      0.0 MiB           1       learningrate,lambda1,lambda2=0.0003,1.5e-2,3e-4\n",
        "    38   1894.1 MiB      0.0 MiB           1       epochs = 50\n",
        "    39   1894.1 MiB      0.0 MiB           1       rootpath=\"./\"\n",
        "    40   1894.1 MiB      0.0 MiB           1       num_classes=400\n",
        "    41   1894.1 MiB      0.0 MiB           1       Centers_Lengths_dimension=4\n",
        "    42   1894.1 MiB      0.0 MiB           1       fibs_input_dim=30\n",
        "    43   1894.1 MiB      0.0 MiB           1       dim_fib=30\n",
        "    44   1894.1 MiB      0.0 MiB           1       dim_ctnl=4\n",
        "    45   1894.1 MiB      0.0 MiB           1       recon_dim=fibs_input_dim\n",
        "    46   1895.1 MiB      1.0 MiB           1       inputs_center_and_length = tf.keras.Input(shape=(Centers_Lengths_dimension,))\n",
        "    47   1895.1 MiB      0.0 MiB           1       inputs_fibs_on_ball = tf.keras.Input(shape=(fibs_input_dim,))\n",
        "    48   1895.1 MiB      0.0 MiB           1       encoding_dim = 4\n",
        "    49                                             \n",
        "    50   1895.1 MiB      0.0 MiB           2       def getmodel():\n",
        "    51   3309.2 MiB   1414.1 MiB           1           encoder=keras.layers.Dense(16, activation='tanh')(inputs_fibs_on_ball)\n",
        "    52   3309.2 MiB      0.0 MiB           1           encoder=keras.layers.Dense(16, activation='tanh')(encoder)\n",
        "    53   3309.2 MiB      0.0 MiB           1           encoder=keras.layers.Dense(8, activation='tanh')(encoder)\n",
        "    54   3309.2 MiB      0.0 MiB           1           encoder=keras.layers.Dense(4, activation='tanh')(encoder)\n",
        "    55   3309.2 MiB      0.0 MiB           1           encoder=keras.layers.Dense(encoding_dim, activation='tanh',name=\"feature\")(encoder)\n",
        "    56   3309.3 MiB      0.1 MiB           1           encoder=keras.Model(inputs=inputs_fibs_on_ball, outputs=encoder)\n",
        "    57                                                 #define decoder network:\n",
        "    58                                         \n",
        "    59   3309.3 MiB      0.0 MiB           1           decoder=keras.layers.Dense(4,activation='tanh')(encoder.outputs[0])\n",
        "    60   3309.3 MiB      0.0 MiB           1           decoder=keras.layers.Dense(8, activation='tanh')(decoder)\n",
        "    61                                                 #decoder = keras.layers.Dropout(0.25)(decoder)\n",
        "    62   3309.4 MiB      0.0 MiB           1           decoder=keras.layers.Dense(16,activation='tanh')(decoder)\n",
        "    63                                                 #decoder = keras.layers.Dropout(0.25)(decoder)\n",
        "    64   3309.4 MiB      0.0 MiB           1           decoder=keras.layers.Dense(16,activation='tanh')(decoder)\n",
        "    65   3309.4 MiB      0.0 MiB           1           decoder=keras.layers.Dense(recon_dim,activation='linear')(decoder)\n",
        "    66   3309.4 MiB      0.0 MiB           1           decoder=keras.Model(inputs=encoder.inputs, outputs=decoder)\n",
        "    67                                                 #===\n",
        "    68   3309.5 MiB      0.1 MiB           1           combined = concatenate([decoder.output, encoder.output,inputs_center_and_length,],axis=1)\n",
        "    69   3309.5 MiB      0.0 MiB           1           fusednet = keras.Model(inputs=[inputs_fibs_on_ball,inputs_center_and_length],outputs=combined)\n",
        "    70   3309.5 MiB      0.0 MiB           1           feature_extractor = keras.Model(\n",
        "    71   3309.5 MiB      0.0 MiB           1              inputs=inputs_fibs_on_ball,\n",
        "    72   3309.6 MiB      0.1 MiB           1              outputs=fusednet.get_layer(name=\"feature\").output,\n",
        "    73                                                 )\n",
        "    74   3309.6 MiB      0.0 MiB           1           return fusednet,feature_extractor,\n",
        "    75   3309.6 MiB      0.0 MiB           1       fusednet,feature_extractor = getmodel()\n",
        "    76                                         \n",
        "    77   3309.6 MiB      0.0 MiB           1       model=fusednet\n",
        "    78   3309.6 MiB      0.0 MiB           1       with tf.GradientTape() as tape:\n",
        "    79   3465.5 MiB    156.0 MiB           1           y_pred = model([x_batch_train[:,0:dim_fib],x_batch_train[:,dim_fib:dim_fib+dim_ctnl]], training=True)\n",
        "    80   3465.5 MiB      0.0 MiB           1       z = y_pred[:,recon_dim:]\n",
        "    81   3485.4 MiB     19.9 MiB           1       gm.fit(z)\n",
        "    82   3608.5 MiB    123.1 MiB           1       gamma = gm.predict_proba(z)\n",
        "    83   3608.5 MiB      0.0 MiB           1       gamma_sum = tf.reduce_sum(gamma, axis=0)\n",
        "    84   3608.6 MiB      0.0 MiB           1       phi = tf.reduce_mean(gamma, axis=0)\n",
        "    85   3608.6 MiB      0.0 MiB           1       mu = gm.means_\n",
        "    86   3608.6 MiB      0.0 MiB           1       Sigma = gm.covariances_\n",
        "    87   3608.6 MiB      0.0 MiB           1       n_features = z.shape[1]\n",
        "    88   3608.6 MiB      0.0 MiB           1       min_vals = tf.linalg.diag(tf.ones(n_features, dtype=tf.float64)) * 1e-6\n",
        "    89   3608.6 MiB      0.0 MiB           1       z_centered = z[:, None, :] - mu[None, :, :]\n",
        "    90   3700.8 MiB     92.2 MiB           1       L = tf.linalg.cholesky(Sigma + min_vals[None, :, :])\n",
        "    91   3700.8 MiB      0.0 MiB           1       log_det_sigma = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=1)\n",
        "    92   3700.9 MiB      0.1 MiB           1       v = tf.linalg.triangular_solve(L, tf.transpose(z_centered, [1, 2, 0]))\n",
        "    93   3700.9 MiB      0.0 MiB           1       v = tf.linalg.triangular_solve(L, tf.transpose(z_centered, [1, 2, 0]))\n",
        "    94   3700.9 MiB      0.0 MiB           1       log_det_sigma = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=1)\n",
        "    95   3700.9 MiB      0.0 MiB           1       d = z_centered.shape[1]\n",
        "    96   3700.9 MiB      0.0 MiB           1       logits = tf.dtypes.cast(tf.math.log(phi[:, None]), dtype=tf.float64) - 0.5 * (\n",
        "    97                                                                 tf.dtypes.cast(tf.reduce_sum(tf.square(v), axis=1), dtype=tf.float64) + tf.dtypes.cast(\n",
        "    98   3701.0 MiB      0.1 MiB           1                       d * tf.math.log(2.0 * np.pi), dtype=tf.float64) + tf.dtypes.cast(log_det_sigma[:, None],\n",
        "    99   3701.0 MiB      0.0 MiB           1                                                                                        dtype=tf.float64))\n",
        "   100   3701.2 MiB      0.1 MiB           1       energies = - tf.reduce_logsumexp(logits, axis=0)\n",
        "   101   3701.2 MiB      0.0 MiB           1       enerloss = tf.reduce_mean(energies)\n",
        "   102   3701.2 MiB      0.0 MiB           1       input = y_batch_train[:, 0:recon_dim]\n",
        "   103   3701.2 MiB      0.0 MiB           1       recon = y_pred[:,0:recon_dim]\n",
        "   104   3708.4 MiB      7.3 MiB           1       reconloss = tf.math.reduce_sum(tf.reduce_mean(tf.square(recon - input), axis=-1))\n",
        "   105   3708.4 MiB      0.0 MiB           1       loss_value=lambda1*enerloss + reconloss\n",
        "\n"
      ],
      "metadata": {
        "id": "7STU2jkUoFSm"
      }
    }
  ]
}