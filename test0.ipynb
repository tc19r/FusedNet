{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNO4DXVZl7e+c5+I6GrkjpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tc19r/FusedNet/blob/main/test0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVyMWZNY_Bc7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import math\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "#from my_classes import DataGenerator\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import  TensorBoard\n",
        "import matplotlib\n",
        "matplotlib.use('pdf')\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.layers import concatenate\n",
        "import tensorflow as tf\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "\n",
        "#Hyper parameters \n",
        "learningrate,lambda1,lambda2=0.0003,1.5e-2,3e-4\n",
        "epochs = 50\n",
        "rootpath=\"./\"\n",
        "num_classes=400\n",
        "Centers_Lengths_dimension=4\n",
        "fibs_input_dim=30\n",
        "dim_fib=30\n",
        "dim_ctnl=4\n",
        "recon_dim=fibs_input_dim\n",
        "\n",
        "#Data\n",
        "#Note that walk will search subdirectories too.\n",
        "rootpath='./'\n",
        "X=np.empty([0,34])\n",
        "for root, dirs, files in os.walk(rootpath):\n",
        "     for filename in [fi for fi in files if fi.endswith(\".pf\")]:\n",
        "        pf=np.fromfile(filename)\n",
        "        pf=pf.reshape(int(len(pf)/34),34)\n",
        "        np.random.shuffle(pf)\n",
        "        X=np.vstack([X,pf])\n",
        "        print(pf.shape)\n",
        "trainset=np.float64(X)\n",
        "\n",
        "#\n",
        "x_train=trainset\n",
        "y_train=trainset[:,0:dim_fib]\n",
        "\n",
        "batch_size=int(np.floor(trainset.shape[0]/10))\n",
        "#Model&Trainning\n",
        "\n",
        "\n",
        "inputs_center_and_length = tf.keras.Input(shape=(Centers_Lengths_dimension,))\n",
        "inputs_fibs_on_ball = tf.keras.Input(shape=(fibs_input_dim,))\n",
        "\n",
        "encoding_dim = 4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#define loss.\n",
        "def getmodel():\n",
        "    encoder=keras.layers.Dense(16, activation='tanh')(inputs_fibs_on_ball)\n",
        "    encoder=keras.layers.Dense(16, activation='tanh')(encoder)\n",
        "    encoder=keras.layers.Dense(8, activation='tanh')(encoder)\n",
        "    encoder=keras.layers.Dense(4, activation='tanh')(encoder)\n",
        "    encoder=keras.layers.Dense(encoding_dim, activation='tanh',name=\"feature\")(encoder)\n",
        "    encoder=keras.Model(inputs=inputs_fibs_on_ball, outputs=encoder)\n",
        "    #define decoder network:\n",
        "\n",
        "    decoder=keras.layers.Dense(4,activation='tanh')(encoder.outputs[0])\n",
        "    decoder=keras.layers.Dense(8, activation='tanh')(decoder)\n",
        "    #decoder = keras.layers.Dropout(0.25)(decoder)\n",
        "    decoder=keras.layers.Dense(16,activation='tanh')(decoder)\n",
        "    #decoder = keras.layers.Dropout(0.25)(decoder)\n",
        "    decoder=keras.layers.Dense(16,activation='tanh')(decoder)\n",
        "    decoder=keras.layers.Dense(recon_dim,activation='linear')(decoder)\n",
        "    decoder=keras.Model(inputs=encoder.inputs, outputs=decoder)\n",
        "    #===\n",
        "    combined = concatenate([decoder.output, encoder.output,inputs_center_and_length,],axis=1)\n",
        "    fusednet = keras.Model(inputs=[inputs_fibs_on_ball,inputs_center_and_length],outputs=combined)\n",
        "    feature_extractor = keras.Model(\n",
        "       inputs=inputs_fibs_on_ball,\n",
        "       outputs=fusednet.get_layer(name=\"feature\").output,\n",
        "    )\n",
        "    return fusednet,feature_extractor,\n",
        "fusednet,feature_extractor = getmodel()\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "\n",
        "import time\n",
        "model=fusednet\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learningrate)\n",
        "#initialize gmm object\n",
        "gm = GaussianMixture(n_components=num_classes, random_state=0,warm_start=True,max_iter=10)\n",
        "\n",
        "import time\n",
        "model=fusednet\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learningrate)\n",
        "lr=10000\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model([x_batch_train[:,0:dim_fib],x_batch_train[:,dim_fib:dim_fib+dim_ctnl]], training=True)\n",
        "           #loss_value = loss_fn(y_batch_train, logits)\n",
        "            recon = y_pred[:,0:recon_dim]\n",
        "            z = y_pred[:,recon_dim:]\n",
        "            gm.fit(z)\n",
        "            print(\"fitting complete\")\n",
        "            j=0\n",
        "            l=j*lr \n",
        "            r=np.minimum((j+1)*lr,batch_size)\n",
        "            gamma = gm.predict_proba(z[l:r,:])\n",
        "            gamma_sum = tf.reduce_sum(gamma, axis=0)\n",
        "            \n",
        "            for j in range(1,int(np.ceil(batch_size/lr))):\n",
        "                l=j*lr \n",
        "                r=np.minimum((j+1)*lr,batch_size)\n",
        "                gamma = gm.predict_proba(z[l:r,:])\n",
        "                gamma_sum += tf.reduce_sum(gamma, axis=0)\n",
        "                #phi = tf.reduce_mean(gamma, axis=0)\n",
        "            phi = gamma_sum/batch_size\n",
        "            mu = gm.means_\n",
        "            Sigma = gm.covariances_\n",
        "            n_features = z.shape[1]\n",
        "            min_vals = tf.linalg.diag(tf.ones(n_features, dtype=tf.float64)) * 1e-6\n",
        "            L = tf.linalg.cholesky(Sigma + min_vals[None, :, :])\n",
        "            z_centered = z[:, None, :] - mu[None, :, :]\n",
        "            v = tf.linalg.triangular_solve(L, tf.transpose(z_centered, [1, 2, 0]))\n",
        "            log_det_sigma = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=1)\n",
        "            d = z.shape[1]\n",
        "            logits = tf.dtypes.cast(tf.math.log(phi[:, None]), dtype=tf.float64) - 0.5 * (\n",
        "                    tf.dtypes.cast(tf.reduce_sum(tf.square(v), axis=1), dtype=tf.float64) + tf.dtypes.cast(\n",
        "                d * tf.math.log(2.0 * np.pi), dtype=tf.float64) + tf.dtypes.cast(log_det_sigma[:, None],\n",
        "                                                                                 dtype=tf.float64))\n",
        "            energies = - tf.reduce_logsumexp(logits, axis=0)\n",
        "            enerloss = tf.reduce_mean(energies)\n",
        "            input = y_batch_train[:, 0:recon_dim]\n",
        "            reconloss = tf.math.reduce_sum(tf.reduce_mean(tf.square(recon - input), axis=-1))\n",
        "            loss_value=lambda1*enerloss + reconloss\n",
        "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "                )\n",
        "                print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
        "                #print(phi)\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "\n",
        "fusednet.save(\"./model.h5\")\n",
        "import pickle\n",
        "\n",
        "with open('gm.pkl','wb') as f:\n",
        "    pickle.dump(gm,f)\n"
      ]
    }
  ]
}